To-do list:

[x] Get the HuggingFace dataset
[x] Get the Cohere API key
[] Explore and Preprocess the dataset
    - Do we need to step-ify the data?
    - Some of the problems are already in a step-ified format, but some are not.
    - Do we want to filter the number of records we use?
[] Perturb the dataset 
    - Which model should we use to perturb the data, if we want to start by exploring off-policy?
    - What prompt should we use to perturb the data?
        - I've noticed that sometimes the model is "rewriting" the user's solution, rather than rephrasing it.
            - I'm not sure whether it's adding material differences or not.
        - Can we (stepify-and-)perturb-and-truncate it one-shot, or do we need to do it in multiple steps?
        - I used the prompt generator tool here: https://console.anthropic.com/dashboard
            - It made a good point that we don't necessarily need to provide the question along with the solution; it should be possible
            to provide the steps of the solution without knowing the question... though I wonder if it would help (or hurt - would it encourage rewriting?) to provide it.

TOMORROW:
- Create the prompt using the aNthropic generator for the "perturb" step.
- Include in the prmopt the taxonomy
- Use the two (or more) examples you already posted in Slack


Meeting to-do:
- Decide how to handle the truncation
    - Ask the model to do it, or try to have a roughly even distribution and do it programmatically?
- Decide which models to test the recovery on
    - Only on non-base models, right?
- Decide how to detect the recovery
- Should I be having 15-25s latency? 
- I used the default temperature of 0.3 for the perturb step, which seems to work well, but should I be using 0 as a temp??


Problems I've been having (and have mostly addressed):
- Model is explicitly including the type of perturbation applied, in the steps (eg [Perturbation: Wrong Formula])
- Model is including the change applied (eg ...is also $1+x > 0$ (instead of $1-x > 0$))
- Problems with adherence to the instructions -- might only output a perturbed chain, and none of the other information.
    - Should I have it select the random step and the perturbation type before generating the perturbed output?
- It seems like the OpenAI Python library used to support a client.chat.completions API that let you 
set the history for user and model... but that's been deprecated. Unsure if I still can

-----

I don't see how it's possible to continue a generation 
using the OpenAI API.

I think I recall seeing recently a post (and 
I think I saved it or took a screnshot of it on twitter) regarding
using the Calude API to continue generations.